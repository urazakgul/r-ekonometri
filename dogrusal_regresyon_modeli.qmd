# Doğrusal Regresyon Modeli

## Genel

Temellerin atıldığı bir konu olduğu için bu bölümün çok önemli olduğunu düşünüyorum. William H. Greene, *"ekonometricinin alet çantasındaki en kullanışlı tek araç doğrusal regresyon modelidir."* der. Doğrusal Regresyon Modeli (DRM) ile ilgili herhangi bir soru işaretinizin kalmadığına emin olmalısınız.

DRM'yi şöyle yazalım:

$Y_i = \beta_1 + \beta_2X_{2i} + ... + \beta_kX_{ki} + \epsilon_i$

$Y:$ Bağımlı ya da açıklanan değişken.

$X:$ Bağımsız ya da açıklayıcı değişken.

$\epsilon:$ Rassal veya stokastik hata terimi. Çeşitli nedenler ile modele konulamayan değişkenleri içerir. Stokastik sözcüğü hedef ya da hedefin göbeği anlamına gelir. Peter Kennedy, stokastik ve hata terimini, *"Stokastik bir ilişki, okun nadiren hedefi tam on ikiden vurması gibi, bağımlı değişken değerinin tam olarak öngörülmesi anlamında, her zaman hedefi vuramaz. Hata terimi açıkça bu hedeften sapmaların ya da hataların büyüklüklerini belirlemek amacı ile kullanılır."* örneği ile açıklar. Hata terimi gibi bir de kalıntı elde edeceğiz. İkisi aynı anlama gelmek ile beraber örneklem söz konusu olduğu zaman kullanılan ifade kalıntı olacaktır. Kennedy, hata terimi için *"ekonometristlerin kullandığı tahmin yöntemlerinin başarısı büyük bir oranda hata teriminin yapısına bağlıdır."* der.

$\beta_1:$ Kesme terimi ya sa sabit terim. Tüm X'ler sıfıra eşitlendiğinde Y'nin ortalama değerini gösterir deriz ancak daha net bir ifadeyle modelde bulunmayan (aşağıda açıklanan hata terimine bakın) bütün değişkenlerin Y üzerindeki ortalama etkisidir.

$\beta_2,...,\beta_k:$ Kısmi eğim parametreleri ya da kısmi regresyon parametreleri.

$\beta_1,\beta_2,...,\beta_k:$ Regresyon parametreleri.

$i:$ i-nci gözlem.

DRM'nin gösterimini aşağıdaki gibi sadeleştirmek mümkündür.

$Y_i = \beta X + \epsilon_i$

Yukarıdaki eşitliklere anakütle modeli denir. Bu model, deterministik bileşen $\beta X$ ile rassal bileşen $\epsilon_i$'nin birleşimidir. $\beta X$ için $X$ değerleri verildiğinde $Y_i$'nin koşullu ortalaması $E(Y_i|X)$ diyebiliriz.

Regresyon analizinde öncelikli amacımız X değişkenlerinin değerlerindeki değişimlere Y'nin verdiği ortalama tepkiyi ölçmektir. Bu noktada eğim parametrelerinden bahsedebiliriz. Eğim parametresi, diğer açıklayıcı değişkenlerin değerleri sabit tutulduğunda bir açıklayıcı değişken değerindeki bir birim değişim karşılığında Y'nin ortalama değerindeki değişimi ölçer. Yeri gelmişken kısmi eğim parametreleri ya da kısmi regresyon parametrelerini açıklayalım. Buradaki kısmilik şuradan gelir: Bir açıklayıcı değişkendeki bir birimlik değişimin Y'nin ortalaması üzerindeki doğrudan ya da net etkisi, diğer açıklayıcı değişkenlerin Y'nin ortalaması üzerinde olabilecek etkisinden arındırılarak ölçülür. Bu nedenle kısmi kavramı kullanılır.

## Sıradan En Küçük Kareler (OLS)

Bu bölümün sonunda bir ücret regresyonu kuracağız. Buna geçmeden önce yukarıda yazdığımız aşağıdaki eşitliğin nasıl tahmin edileceğine bakalım.

$Y_i = \beta_1 + \beta_2X_{2i} + ... + \beta_kX_{ki} + \epsilon_i$

Regresyon parametrelerini tahmin etmede sıradan en küçük kareler (OLS, Ordinary Least Squares) ciddi bir kullanımı olan bir yöntemdir.

$\epsilon_i$ dediğimiz hata terimi, gerçek Y değerleri ile regresyon modelinden elde edilen Y değerleri arasındaki farktır.

$\epsilon_i = Y_i - (\beta_1 + \beta_2X_{2i} + ... + \beta_kX_{ki})$

$\epsilon_i = Y_i - \beta X$

OLS, bu hata teriminin kareler toplamını minimize eder.

$\sum \epsilon_i^2 = \sum (\beta_1 - \beta_2X_{2i} - ... - \beta_kX_{ki})^2$

Hata kareler toplamını mümkün olduğunca küçük yapan $\beta$ parametresi değerleri bulunmalıdır. Tahmin edilen $\beta$ parametrelerini $\theta$ ile gösterelim.

$\hat{Y_i} = \theta_1 + \theta_2X_{2i} + ... + \theta_kX_{ki} + e_i$

Yukarıdaki eşitliği aşağıdaki gibi sadeleştirebiliriz.

$Y_i = \hat{Y_i} + e_i = \theta X + e_i$

Henüz gördüğümüz $\hat{Y_i}$, $\beta X$'in tahmincisidir. $\beta$ parametrelerinin tahmincileri $\theta$ parametreleri; $\epsilon_i$ hata teriminin tahmincisi ise $e_i$'dir.

## Klasik DRM Varsayımları

Klasik DRM'nin varsayımları model kurma sürecinin önemli bir parçası olacaktır. İlgili varsayımlar aşağıdaki gibidir.

* Regresyon modeli parametreler açısından doğrusaldır. Y ve X değişkenlerine göre ise doğrusallık aranmaz.

Parametreler açısından doğrusallık: Parametrelerin kuvveti alınmamış ($\beta_2^2$ gibi), parametreler diğer parametrelere bölünmemiş ($\beta_2 / \beta_3$ gibi) veya dönüştürülmemiştir ($ln\beta_4$ gibi).

Değişkenler açısından: Koşul aranmaz. Örneğin, X değişkeninin doğal logaritması ($lnX_2$ gibi), tersi ($1/X_3$ gibi) veya kuvveti ($X_2^3$ gibi) alınmış olabilir.

* $cov(\epsilon_i,X) = 0$. Değerlerinin tekrarlanmış örneklemlerde sabit olmasına bağlı olarak, açıklayıcı değişkenlerin sabit olduğu veya stokastik olmadığı varsayılır. Bu varsayıma sabit X değerleri ya da hata teriminden bağımsız X değerleri diyebiliriz. Yani, her X değişkeni ile $\epsilon_i$ arasındaki ortak varyans sıfırdır.

* $E(\epsilon_i|X) = 0$. X değişkenlerinin değerleri verildiğinde hata teriminin beklenen ya da ortalama değeri sıfırdır. Bu durumda yazdığımız $Y_i = \beta X + \epsilon_i$ eşitliğini $E(Y_i|X) = \beta X + E(\epsilon_i|X) = \beta X$ şeklinde yazabiliriz.

* $var(\epsilon_i|X) = \sigma^2$. X değerleri verildiğinde her bir $\epsilon_i$'nin varyansı sabittir (sabit varyans).

* $cov(\epsilon_i,\epsilon_j|X) = 0, i\neq j$. İki hata terimi arasında korelasyon (otokorelasyon) yoktur.

* X değişkenleri arasında tam doğrusal ilişki ya da çoklu doğrusal bağlantı yoktur.

* Regresyon modeli doğru tanımlanmış olup herhangi bir tanımlama yanlılığı ya da hatası yoktur.

* Gözlem sayısı tahmin edilecek anakütle parametrelerinden fazla olmalıdır. Daha sade bir anlatım ile gözlem sayısı açıklayıcı değişken sayısından büyük olmalıdır diyebiliriz.

* $\epsilon_i \sim N(0,\sigma^2)$. Hata terimi sıfır ortalamalı ve $\sigma^2$ (sabit) varyanslı normal dağılıma sahiptir.

Ayrıca OLS tahmincileri BLUE'dur. BLUE (Best Linear Unbiased Estimator), en iyi doğrusal yansız tahminci anlamına gelmektedir. Akılda tutmak için Doğrusal En iyi Sapmasız Tahmin Edici olan DESTE de kullanılabilir.

* Doğrusal tahminci: Tahminciler Y bağımlı değişkeninin doğrusal fonksiyonlarıdır.

* Yansız tahminci: Yöntemin tekrarlanan uygulamalarında tahminciler ortalama olarak gerçek değerlerine eşittir.

* En küçük varyansa sahip tahminci / etkin tahminci: Doğrusal yansız tahminciler sınıfı içinde OLS tahmincileri minimum varyansa sahiptir.

## OLS Tahmincilerinin Standart Hataları

$\theta$ OLS tahmincileri, değerleri örnekleme bağlı olarak değiştiği için rassal değişkendir. Bu noktada değişkenliği ölçmemiz gerekmektedir. İstatistikte de bu değişkenlik varyans ($\sigma^2$) ve standart sapma ($\sigma$) ile ölçülür. Bir tahmincinin standart sapması regresyon bağlamında standart hatadır. Standart hata, tahmin edicinin örneklem dağılımının standart sapmasıdır. DRM'de $u_i$ hata terimi varyansına ($\sigma^2$) ait tahmin şöyledir:

$\hat{\sigma}^2 = \frac{\sum e_i^2}{n-k}$

Burada, n örneklem büyüklüğü ilen k tahmin edilen regresyon parametre sayısıdır. $\sqrt{\hat{\sigma}^2}$ ya da $\hat{\sigma}$, regresyonun standart hatası ya da kök ortalama karedir.

## Hipotez Testleri

### t Testi

Anakütle regresyon parametresi için $\beta_k = 0$ hipotezini test edelim.

$H_0: \beta_k = 0$

$H_1: \beta_k \neq 0$

Bu hipotezin testi için ise şunu yazalım:

$t = \frac{\theta_k}{se({\theta_k})}$

Burada t testini kullanyoruz. $se({\theta_k})$, $\theta_k$'nın standart hatasıdır. t değerinin (n-k) serbestlik derecesi vardır. Hesaplanan t değeri olasılığı düşük çıkarsa (%5 veya daha az gibi) $\beta_k = 0$ sıfır hipotezi reddedilir. Sıfır hipotezinin reddi ise t değerinin istatistiksel olarak anlamlı olduğu anlamına gelir. Daha geniş bir ifade ile diğer açıklayıcı değişkenler sabit iken incelenen değişkenin bağımlı değişken üzerinde istatistiksel olarak anlamlı bir etkisinin olduğu söylenebilir. Diyelim ki sıfır hipotezini kabul ettik. Burada edilen kabul, örneklem verilerine göre bu hipotezi reddedecek nedeni henüz bulamadığımızdandır. Yani, bu kabul net doğru anlamına gelmez. Özetle, kabul ederiz yerine reddedemeyiz demek yerinde olacaktır.

### F Testi

t testi ile bireysel anlamlılığa bakıyorduk. Bütün eğim parametrelerinin aynı anda anlamlı olup olmadığına bakmak için ise F testini kullanacağız. Buna regresyonun genel anlamlılığı da diyebiliriz.

$H_0: \beta_2 = \beta_3 = ... = \beta_k = 0$

$H_1:$ En az bir $\beta_i \neq 0$

F istatistiği şöyledir:

$F = \frac{ESS/sd}{RSS/sd} = \frac{Ortalama \ ESS}{Ortalama \ RSS}$

$ESS = \sum (\hat{Y_i} - \bar{Y})^2$

$RSS = \sum (Y_i - \hat{Y_i})$

$TSS = \sum (Y_i - \bar{Y})^2$

ESS, Y bağımlı değişkenindeki değişkenliğin model tarafından açıklanan kısmı iken, RSS, Y bağımlı değişkenindeki değişkenliğin model tarafından açıklanmayan kısmıdır. Y bağımlı değişkenindeki toplam değişkenlik ise TSS olup ESS ile RSS'in toplamıdır. sd, serbestlik derecesidir ve payın serbestlik derecesi k-1 iken, paydanın serbestlik derecesi n-k'dır. Yukarıdaki eşitliği tekrar yazalım.

$F = \frac{ESS}{TSS} = \frac{\sum (\hat{Y_i} - \bar{Y}) / (k - 1)}{\sum (Y_i - \hat{Y_i}) / (n - k)}$

Eğer hesaplayacağımız F değeri $\alpha$ seviyesindeki kritik F değerinden büyük ise sıfır hipotezi reddedilebilir ki bu da en az bir açıklayıcı değişkenin istatistiksel olarak anlamlı olduğu anlamına gelir.

## Güven Aralığı

Bir tek nokta tahminine güvenmek yerine belli bir olasılıkla (örneğin %95) gerçek parametreyi içerecek şekilde bir aralık oluşturabiliriz. Herhangi bir anakütle parametresi $\beta_k$ için ($1-\alpha$) güven aralığı şöyledir:

$Pr[\theta_k \pm t_{\alpha/2}se({\theta_k})] = (1-\alpha)$

$[\theta_k - t_{\alpha/2}se({\theta_k})]$: Alt sınır

$[\theta_k + t_{\alpha/2}se({\theta_k})]$: Üst sınır

Güven aralığının genişliği tahmin edicinin güvenilirliği olan standart hatası ile orantılıdır. Dikkat etmemiz gereken nokta bu güven aralığı gerçek $\beta_k$'nın verilen alt ve üst sınırlar arasında yer alma olasılığının ($1-\alpha$) olduğunu söylemez. Aksine gerçek $\beta_k$ değerini sabit bir sayı kabul ederiz ve bu olasılık da ya 1'dir ya da 0. Asıl söylediği, her 100 aralığın 95'inde (güven katsayısının %95 olduğunu varsayalım) gerçek $\beta_k$'yı içerdiğidir.

## Uyum İyiliği

Tahmin edilen regresyon doğrusunun uyum iyiliğinin ölçüsü $R^2$'dir ve şöyle hesaplanır:

$R^2 = \frac{ESS}{TSS}$ ya da $R^2 = 1 - \frac{RSS}{TSS}$

$R^2$ değeri 0 ile 1 arasında yer alır ($0 \leq R^2 \leq 1$) ve 1'e yaklaştıkça uyum iyileşir. Uyumun iyileşmesi açıklayıcılık gücünün arttığı anlamına gelir. Bu noktada modele açıklayıcı değişken ekledikçe $R^2$ değerinin artacağı bilinmelidir. Bu durumda düzeltilmiş $R^2$ ya da $\bar{R^2}$ kullanılabilir ve şöyle hesaplanır:

$\bar{R^2} = 1 - (1 - R^2) \frac{n-1}{n-k}$

Düzeltilmiş ile serbestlik derecesi düzeltmesi kastedilmektedir. İki nokta:

* $k > 1$ ise $\bar{R^2}$ < $R^2$'dir.

* $R^2$ daima pozitif iken $\bar{R^2}$ negatif olabilir.

Yüksek $\bar{R^2}$ bulma yarışına bir not düşmek gerekir. Bizim asıl ilgimiz, açıklayıcı değişkenlerin bağımlı değişken ile olan mantıksal ilişkilerine ve onların istatistiksel anlamlılıklarına odaklı olmalıdır. Yüksek bir $\bar{R^2}$ bulamasak da bu modelin kötü olduğu anlamına gelmez.

## Uygulama

Saatlik ücreti (dolar bazında) belirleyen faktörleri örneklemde araştırmak
üzere, Mart 1995'te görüşme yapılan 1289 kişilik (anakütleden alınan örneklem) bir yatay-kesite bakalım. İlgili verilere *drm.xls* dosyası ile ulaşılabilir.

Bağımlı değişken:

* **wage:** Saatlik ücret ($)

Bağımsız değişkenler:

* **female:** Kadın ise 1; değilse 0

* **nonwhite:** Beyaz olmayan işçi ise 1; değilse 0

* **union:** Sendikalı bir işte ise 1; değilse 0

* **education:** Yıl bazlı eğitim

* **exper:** Yıl bazlı iş deneyimi. Yaş – eğitim süresi – 6 okula başlama yaşı

```{r}
#| warning: false

library(readxl)
library(tidyverse)
library(magrittr)

```

```{r}

df <- read_excel("./data/drm.xls")

df %<>%
select(wage, female, nonwhite, union, education, exper)

```

DRM'yi kurabiliriz.

```{r}

model <- lm(formula = wage ~ female + nonwhite + union + education + exper, data = df)
#ya da model <- lm(formula = wage ~., data = df)
summary(model)

```

* Regresyondaki açıklayıcı değişkenleri sıfıra eşitlediğimizde ortalama wage -$7.18 olur. Tabi bunun iktisadi açıdan bir anlamı yoktur. Ancak buna rağmen kesme terimini bırakmak faydalı olabilir.

Diğer değişkenler sabit tutulduğunda;

* Kadınların ortalama wage'i erkeklerin ortalama wage'inden $3.07 daha düşüktür (female).

* Beyaz olmayan bir işçinin ortalama wage'i beyaz bir işçinin ortalama wage'inden $1.56 daha düşüktür (nonwhite).

* Sendikalı bir işte çalışanın ortalama wage'i sendikalı bir işte çalışmayanın ortalama wage'inden $1.09 daha fazladır (union).

* Her ilave eğitim yılı için ortalama wage $1.37 artmaktadır (education).

* Her ilave deneyim için ortalama wage $0.16 artmaktadır (exper).

Diğer yorumlara bakalım.

* Bu model yardımı ile bir kişinin alacağı ücreti kesin olarak söyleyemeyiz. Sadece bu kişinin niteliklerine göre ne kazanabileceğini öngörebiliriz.

* p değeri küçüldükçe sıfır hipotezi aleyhindeki kanıtlar daha da güçlenir. Örneğin, yaklaşık 1.37 olan education parametresine ait değerin yaklaşık 20.79 olan bir t değeri hesaplandı. p değeri neredeyse sıfırdır (2e-16). Bu durumda education parametresi istatistiksel olarak oldukça anlamlıdır. Yani, education değişkeni wage değişkeninin önemli bir belirleyicisidir. %5 gibi bir p değeri aldığımızda tüm parametrelerin istatistiksel olarak anlamlı olduğunu görüyoruz. Yani tüm değişkenler wage'in önemli bir belirleyicisidir.

* $R^2$ değeri yaklaşık olarak 0.32'dir. Wage değişkenindeki değişkenliğin yaklaşık %32'si beş açıklayıcı değişken tarafından açıklanmaktadır.

* F değerine ait p değeri neredeyse sıfır (2.2e-16) olduğu için en az bir değişkenin wage değişkeni üzerinde anlamlı bir etkisi vardır. F değerini manuel hesaplayalım. Manuel hesaplarken F değerinin $R^2$ ile olan ilişkisini göreceğiz.

(k-1): Kesme terimi dışarıda tutulduğunda açıklayıcı değişken sayısı (5)

n: Gözlem sayısı 1289 ile kesme terimi dahil tahmin edilen parametre sayısı 6'nın farkı (1283)

$F = \frac{R^2/(k-1)}{(1-R^2)/(n-k)} = \frac{0.3233/5}{(1-0.3233)/(1283)} = 122.6$

* Gerçek education parametresinin en iyi tek tahmini 1.37'dir ama bunu örneğin %95 güven aralığında da yorumlayabiliriz. Aşağıdan da görüleceği üzere, diğer şeyler sabit iken ek 1 yıllık eğitimin wage üzerindeki etkisinin minimum $1.24 ve maksimum $1.49 olduğu konusunda %95 güvendeyiz. Hatırlayalım: gerçek education parametresinin $1.32 olduğunu varsayalım. Bu durumda $1.32 bu aralıkta ya yer alır ya da yer almaz. Olasılık ya 1'dir ya da 0'dır. Aralık, her 100 aralığın 95'inde (güven katsayısının %95 olduğunu varsayalım) gerçek education parametresini içerir. %95 güven katsayısı ile hareket ettiğimizde %5'inde hatalı oluruz.

```{r}

confint(model, level = 0.95)

```

Aşağıdaki değerler ile beklenen ortalama ücreti bulalım.

* female: 1

* nonwhite: 1

* union: 0

* education: 12

* exper: 20

Kadın, beyaz olmayan, sendikası olmayan, 12 yıllık eğitime sahip, 20 yıllık iş deneyimi olan bir işçinin beklenen ortalama ücretine bakıyoruz.

```{r}

df2 <- data.frame(
    female = 1,
    nonwhite = 1,
    union = 0,
    education = 12,
    exper = 20
)

wage_pred <- predict(
    model, newdata = df2
)

paste0("$",round(wage_pred,digits=2))

```